{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(1234)\n",
    "import pickle\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Sort problem. E.g. for problem length 6:\n",
    "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "    output: I I I I I 0 0 0 1 1 2\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6, num_digits=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.num_digits\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # use rejection sampling to generate an input example from the desired split\n",
    "        while True:\n",
    "            # generate some random integers\n",
    "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "            # half of the time let's try to boost the number of examples that \n",
    "            # have a large number of repeats, as this is what the model seems to struggle\n",
    "            # with later in training, and they are kind of rate\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                if inp.unique().nelement() > self.length // 2:\n",
    "                    # too many unqiue digits, re-sample\n",
    "                    continue\n",
    "            # figure out if this generated example is train or test based on its hash\n",
    "            h = hash(pickle.dumps(inp.tolist()))\n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        # solve the task: i.e. sort\n",
    "        sol = torch.sort(inp)[0]\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:self.length-1] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1\n",
      "0 -1\n",
      "0 -1\n",
      "2 -1\n",
      "2 -1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "1 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = SortDataset('train')\n",
    "test_dataset = SortDataset('test')\n",
    "x, y = train_dataset[0]\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.09M\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 200\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trainer on device: cuda\n",
      "iter_dt 0.00ms; iter 0: train loss 1.11904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:343] Profiler is not initialized: skipping step() invocation\n",
      "[W kineto_shim.cpp:157] Cannot run range profiler with CPU activities, please only use CUDA activity type\n",
      "STAGE:2024-01-25 16:09:27 64601:64601 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 47.93ms; iter 100: train loss 0.16789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-25 16:09:40 64601:64601 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-01-25 16:09:41 64601:64601 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/main/torch/profiler/profiler.py\n",
    "\n",
    "from torch.autograd import kineto_available, ProfilerActivity\n",
    "from torch.profiler import profile, schedule, tensorboard_trace_handler\n",
    "\n",
    "tracing_schedule = schedule(skip_first=5, wait=5, warmup=2, active=500, repeat=1)\n",
    "trace_handler = tensorboard_trace_handler(dir_name=\"/scratch/user/siweicui/xllm/kineto/tracing/trace_data/counter_tracing/\", use_gzip=False)\n",
    "\n",
    "\n",
    "# with profile(\n",
    "#   activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#   schedule = tracing_schedule,\n",
    "#   on_trace_ready = trace_handler,\n",
    "#   # profile_memory = True,\n",
    "#   record_shapes = True,\n",
    "#   # with_stack = True,\n",
    "#   experimental_config=torch.profiler._ExperimentalConfig(\n",
    "#         profiler_metrics=[\n",
    "#             \"kineto__tensor_core_insts\",\n",
    "#             \"dram__bytes_read.sum\",\n",
    "#             \"dram__bytes_write.sum\"],\n",
    "#   profiler_measure_per_kernel=True),\n",
    "# ) as prof:\n",
    "with torch.profiler.profile(\n",
    "    activities=[# [W kineto_shim.cpp:157] Cannot run range profiler with CPU activities, please only use CUDA activity type\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "                ],\n",
    "    record_shapes = True,\n",
    "    schedule = tracing_schedule,\n",
    "    on_trace_ready=trace_handler,\n",
    "    with_stack = True,\n",
    "    profile_memory = True,\n",
    "    with_flops=True,\n",
    "    experimental_config=torch.profiler._ExperimentalConfig(\n",
    "        profiler_metrics=[\n",
    "            \"kineto__tensor_core_insts\",\n",
    "            \"dram__bytes_read.sum\",\n",
    "            \"dram__bytes_write.sum\"],\n",
    "    profiler_measure_per_kernel=True),\n",
    ") as prof:\n",
    "    def batch_end_callback(trainer):\n",
    "      if trainer.iter_num % 100 == 0:\n",
    "          print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "      prof.step()\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "    trainer.run()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*        35.34%        4.367s        76.73%        9.481s      50.164ms       0.000us         0.00%     220.835ms       1.168ms           0 b           0 b     -19.50 Kb      -4.11 Gb           189            --  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.76%      93.748ms         5.53%     683.420ms     302.934us       0.000us         0.00%     139.158ms      61.684us           0 b           0 b    -593.19 Mb      -1.14 Gb          2256            --  \n",
      "                                               aten::mm         1.70%     209.765ms         2.55%     314.745ms      62.007us      99.714ms        18.61%     106.041ms      20.891us           0 b           0 b     594.38 Mb     594.38 Mb          5076     44025.569  \n",
      "                                         AddmmBackward0         0.68%      84.283ms         3.78%     466.865ms     206.944us       0.000us         0.00%      87.517ms      38.793us           0 b           0 b     568.41 Mb           0 b          2256            --  \n",
      "                                              aten::sum         0.67%      83.229ms         0.93%     115.255ms      47.158us      53.003ms         9.89%      53.984ms      22.088us           0 b           0 b       2.11 Mb       2.11 Mb          2444            --  \n",
      "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      53.003ms         9.89%      53.003ms      21.687us           0 b           0 b           0 b           0 b          2444            --  \n",
      "                                              aten::bmm         0.91%     112.017ms         1.22%     150.270ms      44.406us      50.967ms         9.51%      52.749ms      15.588us           0 b           0 b     391.05 Mb     391.05 Mb          3384      2515.747  \n",
      "                                              aten::mul         1.40%     172.540ms         2.11%     261.015ms      32.288us      46.661ms         8.71%      50.762ms       6.279us          56 b          56 b       3.27 Gb       3.27 Gb          8084       877.891  \n",
      "                                           aten::linear         0.28%      34.237ms         1.72%     211.974ms      86.732us       0.000us         0.00%      43.698ms      17.880us           0 b           0 b     655.89 Mb      68.00 Kb          2444            --  \n",
      "                                            aten::addmm         0.80%      98.466ms         1.05%     130.276ms      57.746us      38.499ms         7.18%      41.365ms      18.336us           0 b           0 b     654.33 Mb      -1.56 Gb          2256     21955.609  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.357s\n",
      "Self CUDA time total: 535.919ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# https://discuss.pytorch.org/t/understanding-memory-profiler-output-autograd-profiler-with-memory-stats/101704\n",
    "# Negative memory (mostly found in self) indicate deallocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prof.key_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         1.10%     136.486ms         1.89%     234.009ms      17.782us      11.085ms         2.07%      11.475ms       0.872us       3.12 Mb       3.12 Mb     218.20 Mb     218.20 Mb         13160            --  \n",
      "                                    aten::empty_strided         1.31%     161.491ms         1.31%     161.491ms       3.135us       0.000us         0.00%       0.000us       0.000us       1.75 Mb       1.75 Mb     403.45 Mb     403.45 Mb         51512            --  \n",
      "                                            aten::empty         1.59%     196.048ms         1.59%     196.048ms       2.197us       0.000us         0.00%       0.000us       0.000us     997.48 Kb     997.48 Kb       3.35 Gb       3.35 Gb         89228            --  \n",
      "                                             aten::sort         2.22%     274.495ms         3.65%     451.524ms      37.273us       0.000us         0.00%       0.000us       0.000us       1.11 Mb     571.17 Kb           0 b           0 b         12114            --  \n",
      "                                          aten::resize_         0.09%      10.636ms         0.09%      10.636ms       0.832us       0.000us         0.00%       0.000us       0.000us     330.09 Kb     330.09 Kb      24.40 Mb      24.40 Mb         12784            --  \n",
      "                                            aten::clone         1.52%     187.854ms         3.67%     453.061ms      15.548us       0.000us         0.00%      38.434ms       1.319us       2.02 Mb     292.53 Kb     609.16 Mb      -5.18 Mb         29140            --  \n",
      "                                           aten::arange         1.13%     139.679ms         1.89%     234.011ms       9.575us     940.000us         0.18%       1.953ms       0.080us       1.10 Mb     252.75 Kb     188.00 Kb       5.00 Kb         24440            --  \n",
      "                                          aten::randint         0.80%      99.196ms         1.17%     144.881ms       8.507us       0.000us         0.00%       0.000us       0.000us     874.70 Kb      63.84 Kb           0 b           0 b         17031            --  \n",
      "                                         aten::_unique2         0.64%      79.140ms         0.72%      89.413ms      10.693us       0.000us         0.00%       0.000us       0.000us     178.51 Kb      49.78 Kb           0 b           0 b          8362            --  \n",
      "                                             aten::rand         0.69%      85.435ms         0.95%     117.206ms       7.037us       0.000us         0.00%       0.000us       0.000us      65.06 Kb      10.16 Kb           0 b           0 b         16655            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.357s\n",
      "Self CUDA time total: 535.919ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
