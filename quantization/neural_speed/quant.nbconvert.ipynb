{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:17:36.812035Z",
     "iopub.status.busy": "2024-03-18T20:17:36.811594Z",
     "iopub.status.idle": "2024-03-18T20:19:29.689698Z",
     "shell.execute_reply": "2024-03-18T20:19:29.688672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/siweicui/.conda/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:19:29.694735Z",
     "iopub.status.busy": "2024-03-18T20:19:29.694356Z",
     "iopub.status.idle": "2024-03-18T20:20:45.775658Z",
     "shell.execute_reply": "2024-03-18T20:20:45.774606Z"
    }
   },
   "outputs": [],
   "source": [
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "from neural_compressor.quantization import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:20:45.780730Z",
     "iopub.status.busy": "2024-03-18T20:20:45.780290Z",
     "iopub.status.idle": "2024-03-18T20:23:42.594527Z",
     "shell.execute_reply": "2024-03-18T20:23:42.593357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                            | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   5%|█▉                                  | 1/19 [00:07<02:21,  7.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  11%|███▊                                | 2/19 [00:15<02:13,  7.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  16%|█████▋                              | 3/19 [00:22<01:58,  7.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  21%|███████▌                            | 4/19 [00:28<01:44,  6.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  26%|█████████▍                          | 5/19 [00:36<01:39,  7.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  32%|███████████▎                        | 6/19 [00:43<01:34,  7.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  37%|█████████████▎                      | 7/19 [00:50<01:24,  7.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  42%|███████████████▏                    | 8/19 [00:57<01:17,  7.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  47%|█████████████████                   | 9/19 [01:04<01:09,  6.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  53%|██████████████████▍                | 10/19 [01:10<01:00,  6.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  58%|████████████████████▎              | 11/19 [01:17<00:55,  6.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  63%|██████████████████████             | 12/19 [01:24<00:48,  6.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  68%|███████████████████████▉           | 13/19 [01:35<00:49,  8.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  74%|█████████████████████████▊         | 14/19 [01:46<00:45,  9.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  79%|███████████████████████████▋       | 15/19 [01:58<00:39,  9.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  84%|█████████████████████████████▍     | 16/19 [02:07<00:28,  9.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  89%|███████████████████████████████▎   | 17/19 [02:14<00:17,  8.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  95%|█████████████████████████████████▏ | 18/19 [02:26<00:09,  9.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|███████████████████████████████████| 19/19 [02:36<00:00,  9.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|███████████████████████████████████| 19/19 [02:36<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "float_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", cache_dir=\"/scratch/user/siweicui/LLM/huggingface\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:23:42.608568Z",
     "iopub.status.busy": "2024-03-18T20:23:42.608034Z",
     "iopub.status.idle": "2024-03-18T20:25:18.363659Z",
     "shell.execute_reply": "2024-03-18T20:25:18.362573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen! [INST] Do you have mayonnaise recipes? [/INST] Yes, I do have\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "outputs = float_model.generate(inputs, max_new_tokens=5)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:25:18.366805Z",
     "iopub.status.busy": "2024-03-18T20:25:18.366338Z",
     "iopub.status.idle": "2024-03-18T20:25:18.380777Z",
     "shell.execute_reply": "2024-03-18T20:25:18.380164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:25:18.383909Z",
     "iopub.status.busy": "2024-03-18T20:25:18.383665Z",
     "iopub.status.idle": "2024-03-18T20:25:18.386590Z",
     "shell.execute_reply": "2024-03-18T20:25:18.386035Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:25:18.388955Z",
     "iopub.status.busy": "2024-03-18T20:25:18.388679Z",
     "iopub.status.idle": "2024-03-18T20:25:18.391830Z",
     "shell.execute_reply": "2024-03-18T20:25:18.391270Z"
    }
   },
   "outputs": [],
   "source": [
    "woq_conf = PostTrainingQuantConfig(approach=\"weight_only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:25:18.394233Z",
     "iopub.status.busy": "2024-03-18T20:25:18.393969Z",
     "iopub.status.idle": "2024-03-18T20:42:03.758755Z",
     "shell.execute_reply": "2024-03-18T20:42:03.758037Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] Start auto tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] Quantize model without tuning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] Adaptor has 5 recipes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] 0 recipes specified by user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] 3 recipes require future tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] *** Initialize auto tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO] {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]     'PostTrainingQuantConfig': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]         'AccuracyCriterion': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'criterion': 'relative',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'higher_is_better': True,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'tolerable_loss': 0.01,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'absolute': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x2ac9ab5cebd0>>,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:18 [INFO]             'relative': 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'approach': 'post_training_weight_only',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'backend': 'default',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'calibration_sampling_size': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'device': 'cpu',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'diagnosis': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'domain': 'auto',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'excluded_precisions': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'framework': 'pytorch_fx',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'inputs': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'model_name': '',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'ni_workload_name': 'quantization',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'op_name_dict': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'op_type_dict': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'outputs': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'quant_format': 'default',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'quant_level': 'auto',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'recipes': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'smooth_quant': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'smooth_quant_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'layer_wise_quant': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'layer_wise_quant_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'fast_bias_correction': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'weight_correction': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'gemm_to_matmul': True,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'graph_optimization_level': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'first_conv_or_matmul_quantization': True,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'last_conv_or_matmul_quantization': True,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'pre_post_process_quantization': True,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'add_qdq_pair_to_weight': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'optypes_to_exclude_output_quant': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'dedicated_qdq_pair': False,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'rtn_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'awq_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'gptq_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'teq_args': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'reduce_range': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'TuningCriterion': {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'max_trials': 100,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'objective': [\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]                 'performance'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             ],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'strategy': 'basic',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'strategy_kwargs': None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]             'timeout': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]         'use_bf16': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO]     }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] Pass query framework capability elapsed time: 25.23 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] Quantize the model with default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] All algorithms to do: {'RTN'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:25:19 [INFO] quantizing with the round-to-nearest algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] |******Mixed Precision Statistics******|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] +------------+---------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] |  Op Type   |  Total  |    A32W4G32   |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] +------------+---------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] |   Linear   |   929   |      929      |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] +------------+---------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] Pass quantize model elapsed time: 1004043.04 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] Save tuning history to /scratch/user/siweicui/mix_precision/nc_workspace/2024-03-18_15-19-30/./history.snapshot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:42:03 [INFO] Save deploy yaml to /scratch/user/siweicui/mix_precision/nc_workspace/2024-03-18_15-19-30/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "quantized_model = fit(model=float_model, conf=woq_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.761564Z",
     "iopub.status.busy": "2024-03-18T20:42:03.761227Z",
     "iopub.status.idle": "2024-03-18T20:42:03.765880Z",
     "shell.execute_reply": "2024-03-18T20:42:03.765346Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")#.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.768952Z",
     "iopub.status.busy": "2024-03-18T20:42:03.768683Z",
     "iopub.status.idle": "2024-03-18T20:42:03.778833Z",
     "shell.execute_reply": "2024-03-18T20:42:03.778339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTorchFXModel(\n",
       "  (_model): MixtralForCausalLM(\n",
       "    (model): MixtralModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MixtralDecoderLayer(\n",
       "          (self_attn): MixtralAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MixtralRotaryEmbedding()\n",
       "          )\n",
       "          (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "            (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            (experts): ModuleList(\n",
       "              (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "                (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (input_layernorm): MixtralRMSNorm()\n",
       "          (post_attention_layernorm): MixtralRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): MixtralRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.782067Z",
     "iopub.status.busy": "2024-03-18T20:42:03.781789Z",
     "iopub.status.idle": "2024-03-18T20:42:03.792848Z",
     "shell.execute_reply": "2024-03-18T20:42:03.792003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/intel/neural-compressor/issues/1208\n",
    "quantized_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.796126Z",
     "iopub.status.busy": "2024-03-18T20:42:03.795810Z",
     "iopub.status.idle": "2024-03-18T20:42:03.810539Z",
     "shell.execute_reply": "2024-03-18T20:42:03.809973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.fp32_model.eval()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.814190Z",
     "iopub.status.busy": "2024-03-18T20:42:03.813964Z",
     "iopub.status.idle": "2024-03-18T20:42:03.816895Z",
     "shell.execute_reply": "2024-03-18T20:42:03.816342Z"
    }
   },
   "outputs": [],
   "source": [
    "# quantized_model.model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.819338Z",
     "iopub.status.busy": "2024-03-18T20:42:03.819060Z",
     "iopub.status.idle": "2024-03-18T20:42:03.821768Z",
     "shell.execute_reply": "2024-03-18T20:42:03.821208Z"
    }
   },
   "outputs": [],
   "source": [
    "# quantized_model\n",
    "\n",
    "# outputs = quantized_model.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:42:03.824052Z",
     "iopub.status.busy": "2024-03-18T20:42:03.823796Z",
     "iopub.status.idle": "2024-03-18T20:46:28.354377Z",
     "shell.execute_reply": "2024-03-18T20:46:28.323699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:46:27 [INFO] Save config file and weights of quantized model to /scratch/user/siweicui/mix_precision/quantized_model.\n"
     ]
    }
   ],
   "source": [
    "quantized_model.save(\"/scratch/user/siweicui/mix_precision/quantized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:46:29.023534Z",
     "iopub.status.busy": "2024-03-18T20:46:28.968146Z",
     "iopub.status.idle": "2024-03-18T20:47:58.108415Z",
     "shell.execute_reply": "2024-03-18T20:47:58.107197Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen! [INST] Do you have mayonnaise recipes? [/INST] I don't eat\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py\n",
    "outputs = quantized_model.model.generate(inputs, max_new_tokens=5)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:47:58.113139Z",
     "iopub.status.busy": "2024-03-18T20:47:58.112747Z",
     "iopub.status.idle": "2024-03-18T20:47:58.116557Z",
     "shell.execute_reply": "2024-03-18T20:47:58.115917Z"
    }
   },
   "outputs": [],
   "source": [
    "# type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:47:58.119398Z",
     "iopub.status.busy": "2024-03-18T20:47:58.119070Z",
     "iopub.status.idle": "2024-03-18T20:47:58.122170Z",
     "shell.execute_reply": "2024-03-18T20:47:58.121549Z"
    }
   },
   "outputs": [],
   "source": [
    "# quantized_model\n",
    "# q_model = AutoModelForCausalLM.from_pretrained(quantized_model)\n",
    "# outputs2 = quantized_model.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T20:47:58.125090Z",
     "iopub.status.busy": "2024-03-18T20:47:58.124772Z",
     "iopub.status.idle": "2024-03-18T20:47:58.127970Z",
     "shell.execute_reply": "2024-03-18T20:47:58.127331Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(outputs.last_hidden_state[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
