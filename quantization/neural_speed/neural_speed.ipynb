{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/siweicui/.conda/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a little girl,\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, cache_dir=\"/scratch/user/siweicui/LLM/huggingface\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 720/720 [00:00<00:00, 2.50MB/s]\n",
      "2024-03-18 17:28:36 [INFO] Using Pytorch...\n",
      "2024-03-18 17:28:37 [INFO] cpu device is used.\n",
      "2024-03-18 17:28:37 [INFO] Applying Weight Only Quantization.\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [02:47<00:00,  8.79s/it]\n",
      "2024-03-18 17:31:35 [INFO] Start auto tuning.\n",
      "2024-03-18 17:31:35 [INFO] Quantize model without tuning!\n",
      "2024-03-18 17:31:35 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2024-03-18 17:31:35 [INFO] Adaptor has 5 recipes.\n",
      "2024-03-18 17:31:35 [INFO] 0 recipes specified by user.\n",
      "2024-03-18 17:31:35 [INFO] 3 recipes require future tuning.\n",
      "2024-03-18 17:31:36 [INFO] *** Initialize auto tuning\n",
      "2024-03-18 17:31:36 [INFO] {\n",
      "2024-03-18 17:31:36 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-03-18 17:31:36 [INFO]         'AccuracyCriterion': {\n",
      "2024-03-18 17:31:36 [INFO]             'criterion': 'relative',\n",
      "2024-03-18 17:31:36 [INFO]             'higher_is_better': True,\n",
      "2024-03-18 17:31:36 [INFO]             'tolerable_loss': 0.01,\n",
      "2024-03-18 17:31:36 [INFO]             'absolute': None,\n",
      "2024-03-18 17:31:36 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7ff30bded750>>,\n",
      "2024-03-18 17:31:36 [INFO]             'relative': 0.01\n",
      "2024-03-18 17:31:36 [INFO]         },\n",
      "2024-03-18 17:31:36 [INFO]         'approach': 'post_training_weight_only',\n",
      "2024-03-18 17:31:36 [INFO]         'backend': 'default',\n",
      "2024-03-18 17:31:36 [INFO]         'calibration_sampling_size': [\n",
      "2024-03-18 17:31:36 [INFO]             100\n",
      "2024-03-18 17:31:36 [INFO]         ],\n",
      "2024-03-18 17:31:36 [INFO]         'device': 'cpu',\n",
      "2024-03-18 17:31:36 [INFO]         'diagnosis': False,\n",
      "2024-03-18 17:31:36 [INFO]         'domain': 'auto',\n",
      "2024-03-18 17:31:36 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-03-18 17:31:36 [INFO]         'excluded_precisions': [\n",
      "2024-03-18 17:31:36 [INFO]         ],\n",
      "2024-03-18 17:31:36 [INFO]         'framework': 'pytorch_fx',\n",
      "2024-03-18 17:31:36 [INFO]         'inputs': [\n",
      "2024-03-18 17:31:36 [INFO]         ],\n",
      "2024-03-18 17:31:36 [INFO]         'model_name': '',\n",
      "2024-03-18 17:31:36 [INFO]         'ni_workload_name': 'quantization',\n",
      "2024-03-18 17:31:36 [INFO]         'op_name_dict': {\n",
      "2024-03-18 17:31:36 [INFO]             '.*lm_head': {\n",
      "2024-03-18 17:31:36 [INFO]                 'weight': {\n",
      "2024-03-18 17:31:36 [INFO]                     'dtype': [\n",
      "2024-03-18 17:31:36 [INFO]                         'fp32'\n",
      "2024-03-18 17:31:36 [INFO]                     ]\n",
      "2024-03-18 17:31:36 [INFO]                 }\n",
      "2024-03-18 17:31:36 [INFO]             }\n",
      "2024-03-18 17:31:36 [INFO]         },\n",
      "2024-03-18 17:31:36 [INFO]         'op_type_dict': {\n",
      "2024-03-18 17:31:36 [INFO]             '.*': {\n",
      "2024-03-18 17:31:36 [INFO]                 'weight': {\n",
      "2024-03-18 17:31:36 [INFO]                     'bits': [\n",
      "2024-03-18 17:31:36 [INFO]                         4\n",
      "2024-03-18 17:31:36 [INFO]                     ],\n",
      "2024-03-18 17:31:36 [INFO]                     'dtype': [\n",
      "2024-03-18 17:31:36 [INFO]                         'nf4'\n",
      "2024-03-18 17:31:36 [INFO]                     ],\n",
      "2024-03-18 17:31:36 [INFO]                     'group_size': [\n",
      "2024-03-18 17:31:36 [INFO]                         32\n",
      "2024-03-18 17:31:36 [INFO]                     ],\n",
      "2024-03-18 17:31:36 [INFO]                     'scheme': [\n",
      "2024-03-18 17:31:36 [INFO]                         'sym'\n",
      "2024-03-18 17:31:36 [INFO]                     ],\n",
      "2024-03-18 17:31:36 [INFO]                     'algorithm': [\n",
      "2024-03-18 17:31:36 [INFO]                         'RTN'\n",
      "2024-03-18 17:31:36 [INFO]                     ]\n",
      "2024-03-18 17:31:36 [INFO]                 }\n",
      "2024-03-18 17:31:36 [INFO]             }\n",
      "2024-03-18 17:31:36 [INFO]         },\n",
      "2024-03-18 17:31:36 [INFO]         'outputs': [\n",
      "2024-03-18 17:31:36 [INFO]         ],\n",
      "2024-03-18 17:31:36 [INFO]         'quant_format': 'default',\n",
      "2024-03-18 17:31:36 [INFO]         'quant_level': 'auto',\n",
      "2024-03-18 17:31:36 [INFO]         'recipes': {\n",
      "2024-03-18 17:31:36 [INFO]             'smooth_quant': False,\n",
      "2024-03-18 17:31:36 [INFO]             'smooth_quant_args': {\n",
      "2024-03-18 17:31:36 [INFO]             },\n",
      "2024-03-18 17:31:36 [INFO]             'layer_wise_quant': False,\n",
      "2024-03-18 17:31:36 [INFO]             'layer_wise_quant_args': {\n",
      "2024-03-18 17:31:36 [INFO]             },\n",
      "2024-03-18 17:31:36 [INFO]             'fast_bias_correction': False,\n",
      "2024-03-18 17:31:36 [INFO]             'weight_correction': False,\n",
      "2024-03-18 17:31:36 [INFO]             'gemm_to_matmul': True,\n",
      "2024-03-18 17:31:36 [INFO]             'graph_optimization_level': None,\n",
      "2024-03-18 17:31:36 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-03-18 17:31:36 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-03-18 17:31:36 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-03-18 17:31:36 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-03-18 17:31:36 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-03-18 17:31:36 [INFO]             ],\n",
      "2024-03-18 17:31:36 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-03-18 17:31:36 [INFO]             'rtn_args': {\n",
      "2024-03-18 17:31:36 [INFO]                 'enable_full_range': False,\n",
      "2024-03-18 17:31:36 [INFO]                 'enable_mse_search': False\n",
      "2024-03-18 17:31:36 [INFO]             },\n",
      "2024-03-18 17:31:36 [INFO]             'awq_args': {\n",
      "2024-03-18 17:31:36 [INFO]             },\n",
      "2024-03-18 17:31:36 [INFO]             'gptq_args': {\n",
      "2024-03-18 17:31:36 [INFO]             },\n",
      "2024-03-18 17:31:36 [INFO]             'teq_args': {\n",
      "2024-03-18 17:31:36 [INFO]             }\n",
      "2024-03-18 17:31:36 [INFO]         },\n",
      "2024-03-18 17:31:36 [INFO]         'reduce_range': None,\n",
      "2024-03-18 17:31:36 [INFO]         'TuningCriterion': {\n",
      "2024-03-18 17:31:36 [INFO]             'max_trials': 100,\n",
      "2024-03-18 17:31:36 [INFO]             'objective': [\n",
      "2024-03-18 17:31:36 [INFO]                 'performance'\n",
      "2024-03-18 17:31:36 [INFO]             ],\n",
      "2024-03-18 17:31:36 [INFO]             'strategy': 'basic',\n",
      "2024-03-18 17:31:36 [INFO]             'strategy_kwargs': None,\n",
      "2024-03-18 17:31:36 [INFO]             'timeout': 0\n",
      "2024-03-18 17:31:36 [INFO]         },\n",
      "2024-03-18 17:31:36 [INFO]         'use_bf16': True\n",
      "2024-03-18 17:31:36 [INFO]     }\n",
      "2024-03-18 17:31:36 [INFO] }\n",
      "2024-03-18 17:31:36 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-03-18 17:31:36 [INFO] Pass query framework capability elapsed time: 34.23 ms\n",
      "2024-03-18 17:31:36 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2024-03-18 17:31:36 [INFO] Quantize the model with default config.\n",
      "2024-03-18 17:31:36 [INFO] All algorithms to do: {'RTN'}\n",
      "2024-03-18 17:31:36 [INFO] quantizing with the round-to-nearest algorithm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, cache_dir=\"/scratch/user/siweicui/LLM/huggingface\")\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
