Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
> setting tensorboard ...
using world size: 2, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['/scratch/user/siweicui/Megatron-LM/experiments/codeparrot_0_content_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 12
  encoder_seq_length .............................. 1024
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 200
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 3072
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 192
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 768
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0005
  lr_decay_iters .................................. 1500
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 200
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /scratch/user/siweicui/Megatron-LM/merges.txt
  micro_batch_size ................................ 12
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 12
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 12
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_verify_neighbor_count ..................... True
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  save_interval ................................... 200
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. experiments/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_cfg ............................. None
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 1500
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /scratch/user/siweicui/Megatron-LM/vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 2
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/scratch/user/siweicui/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/scratch/user/siweicui/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.159 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 0.456 seconds
/scratch/user/siweicui/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/scratch/user/siweicui/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/scratch/user/siweicui/Megatron-LM/megatron/training.py:125: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/scratch/user/siweicui/Megatron-LM/megatron/training.py:125: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
time to initialize megatron (seconds): 3.753
[after megatron is initialized] datetime: 2023-12-09 10:13:03 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 124475904
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (124475904 elements):
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.position_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.word_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.weight
> learning rate decay style: cosine
WARNING: could not find the metadata file /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (0.57, 0.58)
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-12-09 10:13:03 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      288000
    validation: 15360
    test:       1920
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_vector = [0.969, 0.03, 0.001]
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /scratch/user/siweicui/Megatron-LM/experiments/codeparrot_0_content_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 165625
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 165625
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to a8a36eec9321cf6294323608a2edcff2-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to a8a36eec9321cf6294323608a2edcff2-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to a8a36eec9321cf6294323608a2edcff2-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from a8a36eec9321cf6294323608a2edcff2-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from a8a36eec9321cf6294323608a2edcff2-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from a8a36eec9321cf6294323608a2edcff2-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 726035
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Build and save the document index to 9933d49d1ec89d001a8547e199c984ba-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the sample index to 9933d49d1ec89d001a8547e199c984ba-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Build and save the shuffle index to 9933d49d1ec89d001a8547e199c984ba-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 9933d49d1ec89d001a8547e199c984ba-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 9933d49d1ec89d001a8547e199c984ba-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 9933d49d1ec89d001a8547e199c984ba-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 21682
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 9d8473cedb3a74de6470a41ef6852381-GPTDataset-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 9d8473cedb3a74de6470a41ef6852381-GPTDataset-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 9d8473cedb3a74de6470a41ef6852381-GPTDataset-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1975
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 3
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-12-09 10:13:03 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (115.11, 154.09)
    train/valid/test-data-iterators-setup ..........: (266.30, 266.94)
[before the start of training step] datetime: 2023-12-09 10:13:04 
 iteration       10/    1500 | consumed samples:         1920 | elapsed time per iteration (ms): 1511.4 | learning rate: 0.000E+00 | global batch size:   192 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |
 iteration       20/    1500 | consumed samples:         3840 | elapsed time per iteration (ms): 1278.1 | learning rate: 7.500E-06 | global batch size:   192 | lm loss: 1.028758E+01 | loss scale: 65536.0 | grad norm: 67.788 | number of skipped iterations:   7 | number of nan iterations:   0 |
[Rank 0] (after 20 iterations) memory (MB) | allocated: 2687.71337890625 | max allocated: 20468.99267578125 | reserved: 22414.0 | max reserved: 22414.0
 iteration       30/    1500 | consumed samples:         5760 | elapsed time per iteration (ms): 1283.8 | learning rate: 3.250E-05 | global batch size:   192 | lm loss: 7.177777E+00 | loss scale: 65536.0 | grad norm: 8.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/    1500 | consumed samples:         7680 | elapsed time per iteration (ms): 1273.8 | learning rate: 5.750E-05 | global batch size:   192 | lm loss: 6.191748E+00 | loss scale: 65536.0 | grad norm: 1.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/    1500 | consumed samples:         9600 | elapsed time per iteration (ms): 1274.4 | learning rate: 8.250E-05 | global batch size:   192 | lm loss: 5.655798E+00 | loss scale: 65536.0 | grad norm: 1.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/    1500 | consumed samples:        11520 | elapsed time per iteration (ms): 1277.0 | learning rate: 1.075E-04 | global batch size:   192 | lm loss: 5.062400E+00 | loss scale: 65536.0 | grad norm: 2.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/    1500 | consumed samples:        13440 | elapsed time per iteration (ms): 1276.8 | learning rate: 1.325E-04 | global batch size:   192 | lm loss: 4.602620E+00 | loss scale: 65536.0 | grad norm: 1.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/    1500 | consumed samples:        15360 | elapsed time per iteration (ms): 1282.1 | learning rate: 1.575E-04 | global batch size:   192 | lm loss: 4.294790E+00 | loss scale: 65536.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/    1500 | consumed samples:        17280 | elapsed time per iteration (ms): 1303.1 | learning rate: 1.825E-04 | global batch size:   192 | lm loss: 4.024021E+00 | loss scale: 65536.0 | grad norm: 0.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/    1500 | consumed samples:        19200 | elapsed time per iteration (ms): 1279.5 | learning rate: 2.075E-04 | global batch size:   192 | lm loss: 3.963810E+00 | loss scale: 65536.0 | grad norm: 1.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/    1500 | consumed samples:        21120 | elapsed time per iteration (ms): 1289.6 | learning rate: 2.325E-04 | global batch size:   192 | lm loss: 3.866384E+00 | loss scale: 65536.0 | grad norm: 1.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/    1500 | consumed samples:        23040 | elapsed time per iteration (ms): 1275.4 | learning rate: 2.575E-04 | global batch size:   192 | lm loss: 3.778887E+00 | loss scale: 65536.0 | grad norm: 0.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/    1500 | consumed samples:        24960 | elapsed time per iteration (ms): 1273.3 | learning rate: 2.825E-04 | global batch size:   192 | lm loss: 3.710988E+00 | loss scale: 65536.0 | grad norm: 0.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/    1500 | consumed samples:        26880 | elapsed time per iteration (ms): 1272.7 | learning rate: 3.075E-04 | global batch size:   192 | lm loss: 3.680422E+00 | loss scale: 65536.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/    1500 | consumed samples:        28800 | elapsed time per iteration (ms): 1273.3 | learning rate: 3.325E-04 | global batch size:   192 | lm loss: 3.597161E+00 | loss scale: 65536.0 | grad norm: 0.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/    1500 | consumed samples:        30720 | elapsed time per iteration (ms): 1273.1 | learning rate: 3.575E-04 | global batch size:   192 | lm loss: 3.534626E+00 | loss scale: 65536.0 | grad norm: 0.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/    1500 | consumed samples:        32640 | elapsed time per iteration (ms): 1278.0 | learning rate: 3.825E-04 | global batch size:   192 | lm loss: 3.510788E+00 | loss scale: 65536.0 | grad norm: 0.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/    1500 | consumed samples:        34560 | elapsed time per iteration (ms): 1275.8 | learning rate: 4.075E-04 | global batch size:   192 | lm loss: 3.399886E+00 | loss scale: 65536.0 | grad norm: 0.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/    1500 | consumed samples:        36480 | elapsed time per iteration (ms): 1271.7 | learning rate: 4.325E-04 | global batch size:   192 | lm loss: 3.395951E+00 | loss scale: 65536.0 | grad norm: 1.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/    1500 | consumed samples:        38400 | elapsed time per iteration (ms): 1279.9 | learning rate: 4.575E-04 | global batch size:   192 | lm loss: 3.382934E+00 | loss scale: 65536.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (5169.63, 5169.90)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 3.269923E+00 | lm loss PPL: 2.630931E+01 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     200 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration     200 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3121.25, 3121.30)
 iteration      210/    1500 | consumed samples:        40320 | elapsed time per iteration (ms): 1278.1 | learning rate: 4.825E-04 | global batch size:   192 | lm loss: 3.292631E+00 | loss scale: 65536.0 | grad norm: 0.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      220/    1500 | consumed samples:        42240 | elapsed time per iteration (ms): 1275.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 3.258391E+00 | loss scale: 65536.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      230/    1500 | consumed samples:        44160 | elapsed time per iteration (ms): 1270.2 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 3.209437E+00 | loss scale: 65536.0 | grad norm: 0.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      240/    1500 | consumed samples:        46080 | elapsed time per iteration (ms): 1273.7 | learning rate: 4.996E-04 | global batch size:   192 | lm loss: 3.165989E+00 | loss scale: 65536.0 | grad norm: 1.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      250/    1500 | consumed samples:        48000 | elapsed time per iteration (ms): 1397.2 | learning rate: 4.992E-04 | global batch size:   192 | lm loss: 3.152091E+00 | loss scale: 65536.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      260/    1500 | consumed samples:        49920 | elapsed time per iteration (ms): 1277.2 | learning rate: 4.987E-04 | global batch size:   192 | lm loss: 3.060290E+00 | loss scale: 65536.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      270/    1500 | consumed samples:        51840 | elapsed time per iteration (ms): 1270.4 | learning rate: 4.980E-04 | global batch size:   192 | lm loss: 3.006513E+00 | loss scale: 65536.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      280/    1500 | consumed samples:        53760 | elapsed time per iteration (ms): 1272.0 | learning rate: 4.971E-04 | global batch size:   192 | lm loss: 2.927390E+00 | loss scale: 65536.0 | grad norm: 0.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      290/    1500 | consumed samples:        55680 | elapsed time per iteration (ms): 1271.3 | learning rate: 4.961E-04 | global batch size:   192 | lm loss: 2.916299E+00 | loss scale: 65536.0 | grad norm: 0.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      300/    1500 | consumed samples:        57600 | elapsed time per iteration (ms): 1281.3 | learning rate: 4.950E-04 | global batch size:   192 | lm loss: 2.845356E+00 | loss scale: 65536.0 | grad norm: 0.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      310/    1500 | consumed samples:        59520 | elapsed time per iteration (ms): 1927.8 | learning rate: 4.937E-04 | global batch size:   192 | lm loss: 2.802914E+00 | loss scale: 65536.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      320/    1500 | consumed samples:        61440 | elapsed time per iteration (ms): 1268.0 | learning rate: 4.923E-04 | global batch size:   192 | lm loss: 2.782124E+00 | loss scale: 65536.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      330/    1500 | consumed samples:        63360 | elapsed time per iteration (ms): 1399.4 | learning rate: 4.907E-04 | global batch size:   192 | lm loss: 2.687765E+00 | loss scale: 65536.0 | grad norm: 0.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      340/    1500 | consumed samples:        65280 | elapsed time per iteration (ms): 3438.9 | learning rate: 4.890E-04 | global batch size:   192 | lm loss: 2.703852E+00 | loss scale: 65536.0 | grad norm: 0.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      350/    1500 | consumed samples:        67200 | elapsed time per iteration (ms): 1262.6 | learning rate: 4.872E-04 | global batch size:   192 | lm loss: 2.659994E+00 | loss scale: 65536.0 | grad norm: 1.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      360/    1500 | consumed samples:        69120 | elapsed time per iteration (ms): 2034.3 | learning rate: 4.852E-04 | global batch size:   192 | lm loss: 2.613125E+00 | loss scale: 65536.0 | grad norm: 0.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      370/    1500 | consumed samples:        71040 | elapsed time per iteration (ms): 1813.6 | learning rate: 4.831E-04 | global batch size:   192 | lm loss: 2.569888E+00 | loss scale: 65536.0 | grad norm: 0.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      380/    1500 | consumed samples:        72960 | elapsed time per iteration (ms): 1270.4 | learning rate: 4.809E-04 | global batch size:   192 | lm loss: 2.560538E+00 | loss scale: 65536.0 | grad norm: 0.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      390/    1500 | consumed samples:        74880 | elapsed time per iteration (ms): 1274.2 | learning rate: 4.785E-04 | global batch size:   192 | lm loss: 2.485398E+00 | loss scale: 65536.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      400/    1500 | consumed samples:        76800 | elapsed time per iteration (ms): 1311.1 | learning rate: 4.759E-04 | global batch size:   192 | lm loss: 2.438114E+00 | loss scale: 65536.0 | grad norm: 0.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4948.07, 4948.34)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 2.399463E+00 | lm loss PPL: 1.101726E+01 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     400 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration     400 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3108.14, 3108.23)
 iteration      410/    1500 | consumed samples:        78720 | elapsed time per iteration (ms): 1275.4 | learning rate: 4.733E-04 | global batch size:   192 | lm loss: 2.427045E+00 | loss scale: 65536.0 | grad norm: 0.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      420/    1500 | consumed samples:        80640 | elapsed time per iteration (ms): 1269.3 | learning rate: 4.705E-04 | global batch size:   192 | lm loss: 2.383641E+00 | loss scale: 65536.0 | grad norm: 0.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      430/    1500 | consumed samples:        82560 | elapsed time per iteration (ms): 1375.3 | learning rate: 4.676E-04 | global batch size:   192 | lm loss: 2.365597E+00 | loss scale: 65536.0 | grad norm: 0.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      440/    1500 | consumed samples:        84480 | elapsed time per iteration (ms): 1270.0 | learning rate: 4.646E-04 | global batch size:   192 | lm loss: 2.337040E+00 | loss scale: 65536.0 | grad norm: 0.659 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      450/    1500 | consumed samples:        86400 | elapsed time per iteration (ms): 1271.0 | learning rate: 4.614E-04 | global batch size:   192 | lm loss: 2.308350E+00 | loss scale: 65536.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      460/    1500 | consumed samples:        88320 | elapsed time per iteration (ms): 1285.2 | learning rate: 4.581E-04 | global batch size:   192 | lm loss: 2.274379E+00 | loss scale: 65536.0 | grad norm: 0.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      470/    1500 | consumed samples:        90240 | elapsed time per iteration (ms): 1271.0 | learning rate: 4.547E-04 | global batch size:   192 | lm loss: 2.217864E+00 | loss scale: 65536.0 | grad norm: 0.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      480/    1500 | consumed samples:        92160 | elapsed time per iteration (ms): 1270.6 | learning rate: 4.512E-04 | global batch size:   192 | lm loss: 2.169444E+00 | loss scale: 65536.0 | grad norm: 0.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      490/    1500 | consumed samples:        94080 | elapsed time per iteration (ms): 1295.1 | learning rate: 4.475E-04 | global batch size:   192 | lm loss: 2.123892E+00 | loss scale: 65536.0 | grad norm: 0.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      500/    1500 | consumed samples:        96000 | elapsed time per iteration (ms): 1268.9 | learning rate: 4.438E-04 | global batch size:   192 | lm loss: 2.126605E+00 | loss scale: 65536.0 | grad norm: 0.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      510/    1500 | consumed samples:        97920 | elapsed time per iteration (ms): 1270.7 | learning rate: 4.399E-04 | global batch size:   192 | lm loss: 2.055992E+00 | loss scale: 65536.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      520/    1500 | consumed samples:        99840 | elapsed time per iteration (ms): 1270.1 | learning rate: 4.359E-04 | global batch size:   192 | lm loss: 1.993750E+00 | loss scale: 65536.0 | grad norm: 0.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      530/    1500 | consumed samples:       101760 | elapsed time per iteration (ms): 1272.6 | learning rate: 4.318E-04 | global batch size:   192 | lm loss: 1.968140E+00 | loss scale: 65536.0 | grad norm: 0.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      540/    1500 | consumed samples:       103680 | elapsed time per iteration (ms): 2027.5 | learning rate: 4.276E-04 | global batch size:   192 | lm loss: 1.917055E+00 | loss scale: 65536.0 | grad norm: 0.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      550/    1500 | consumed samples:       105600 | elapsed time per iteration (ms): 1276.8 | learning rate: 4.233E-04 | global batch size:   192 | lm loss: 1.895194E+00 | loss scale: 65536.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      560/    1500 | consumed samples:       107520 | elapsed time per iteration (ms): 1268.9 | learning rate: 4.189E-04 | global batch size:   192 | lm loss: 1.869750E+00 | loss scale: 65536.0 | grad norm: 0.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      570/    1500 | consumed samples:       109440 | elapsed time per iteration (ms): 1269.4 | learning rate: 4.144E-04 | global batch size:   192 | lm loss: 1.861216E+00 | loss scale: 65536.0 | grad norm: 0.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      580/    1500 | consumed samples:       111360 | elapsed time per iteration (ms): 1278.3 | learning rate: 4.098E-04 | global batch size:   192 | lm loss: 1.811893E+00 | loss scale: 65536.0 | grad norm: 0.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      590/    1500 | consumed samples:       113280 | elapsed time per iteration (ms): 1270.0 | learning rate: 4.051E-04 | global batch size:   192 | lm loss: 1.805233E+00 | loss scale: 65536.0 | grad norm: 0.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      600/    1500 | consumed samples:       115200 | elapsed time per iteration (ms): 1344.3 | learning rate: 4.003E-04 | global batch size:   192 | lm loss: 1.774289E+00 | loss scale: 65536.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4962.79, 4962.99)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 600 | lm loss value: 1.709945E+00 | lm loss PPL: 5.528659E+00 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     600 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration     600 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3224.09, 3224.13)
 iteration      610/    1500 | consumed samples:       117120 | elapsed time per iteration (ms): 1273.6 | learning rate: 3.955E-04 | global batch size:   192 | lm loss: 1.766540E+00 | loss scale: 65536.0 | grad norm: 0.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      620/    1500 | consumed samples:       119040 | elapsed time per iteration (ms): 1267.4 | learning rate: 3.905E-04 | global batch size:   192 | lm loss: 1.731967E+00 | loss scale: 65536.0 | grad norm: 0.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      630/    1500 | consumed samples:       120960 | elapsed time per iteration (ms): 1294.2 | learning rate: 3.855E-04 | global batch size:   192 | lm loss: 1.718308E+00 | loss scale: 65536.0 | grad norm: 0.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      640/    1500 | consumed samples:       122880 | elapsed time per iteration (ms): 1271.0 | learning rate: 3.804E-04 | global batch size:   192 | lm loss: 1.703409E+00 | loss scale: 65536.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      650/    1500 | consumed samples:       124800 | elapsed time per iteration (ms): 1269.4 | learning rate: 3.752E-04 | global batch size:   192 | lm loss: 1.687261E+00 | loss scale: 65536.0 | grad norm: 0.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      660/    1500 | consumed samples:       126720 | elapsed time per iteration (ms): 1269.2 | learning rate: 3.699E-04 | global batch size:   192 | lm loss: 1.684461E+00 | loss scale: 65536.0 | grad norm: 0.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      670/    1500 | consumed samples:       128640 | elapsed time per iteration (ms): 1270.3 | learning rate: 3.646E-04 | global batch size:   192 | lm loss: 1.653254E+00 | loss scale: 65536.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      680/    1500 | consumed samples:       130560 | elapsed time per iteration (ms): 1269.5 | learning rate: 3.592E-04 | global batch size:   192 | lm loss: 1.643467E+00 | loss scale: 65536.0 | grad norm: 0.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      690/    1500 | consumed samples:       132480 | elapsed time per iteration (ms): 1270.4 | learning rate: 3.537E-04 | global batch size:   192 | lm loss: 1.659044E+00 | loss scale: 65536.0 | grad norm: 0.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      700/    1500 | consumed samples:       134400 | elapsed time per iteration (ms): 1269.0 | learning rate: 3.482E-04 | global batch size:   192 | lm loss: 1.614508E+00 | loss scale: 65536.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      710/    1500 | consumed samples:       136320 | elapsed time per iteration (ms): 1269.0 | learning rate: 3.426E-04 | global batch size:   192 | lm loss: 1.611169E+00 | loss scale: 65536.0 | grad norm: 0.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      720/    1500 | consumed samples:       138240 | elapsed time per iteration (ms): 1354.0 | learning rate: 3.370E-04 | global batch size:   192 | lm loss: 1.592681E+00 | loss scale: 65536.0 | grad norm: 0.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      730/    1500 | consumed samples:       140160 | elapsed time per iteration (ms): 1292.1 | learning rate: 3.313E-04 | global batch size:   192 | lm loss: 1.598110E+00 | loss scale: 65536.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      740/    1500 | consumed samples:       142080 | elapsed time per iteration (ms): 1274.5 | learning rate: 3.255E-04 | global batch size:   192 | lm loss: 1.601206E+00 | loss scale: 65536.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      750/    1500 | consumed samples:       144000 | elapsed time per iteration (ms): 1272.5 | learning rate: 3.197E-04 | global batch size:   192 | lm loss: 1.570675E+00 | loss scale: 65536.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      760/    1500 | consumed samples:       145920 | elapsed time per iteration (ms): 1274.0 | learning rate: 3.139E-04 | global batch size:   192 | lm loss: 1.581573E+00 | loss scale: 65536.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      770/    1500 | consumed samples:       147840 | elapsed time per iteration (ms): 1269.8 | learning rate: 3.081E-04 | global batch size:   192 | lm loss: 1.566138E+00 | loss scale: 65536.0 | grad norm: 0.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      780/    1500 | consumed samples:       149760 | elapsed time per iteration (ms): 1268.0 | learning rate: 3.022E-04 | global batch size:   192 | lm loss: 1.563128E+00 | loss scale: 65536.0 | grad norm: 0.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      790/    1500 | consumed samples:       151680 | elapsed time per iteration (ms): 1268.6 | learning rate: 2.963E-04 | global batch size:   192 | lm loss: 1.564070E+00 | loss scale: 65536.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      800/    1500 | consumed samples:       153600 | elapsed time per iteration (ms): 1269.2 | learning rate: 2.903E-04 | global batch size:   192 | lm loss: 1.550931E+00 | loss scale: 65536.0 | grad norm: 0.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4832.15, 4832.60)
-----------------------------------------------------------------------------------------------
 validation loss at iteration 800 | lm loss value: 1.448633E+00 | lm loss PPL: 4.257290E+00 | 
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     800 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration     800 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3305.95, 3306.28)
 iteration      810/    1500 | consumed samples:       155520 | elapsed time per iteration (ms): 1273.3 | learning rate: 2.843E-04 | global batch size:   192 | lm loss: 1.532291E+00 | loss scale: 65536.0 | grad norm: 0.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      820/    1500 | consumed samples:       157440 | elapsed time per iteration (ms): 1272.4 | learning rate: 2.783E-04 | global batch size:   192 | lm loss: 1.534474E+00 | loss scale: 65536.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      830/    1500 | consumed samples:       159360 | elapsed time per iteration (ms): 1267.7 | learning rate: 2.723E-04 | global batch size:   192 | lm loss: 1.531671E+00 | loss scale: 65536.0 | grad norm: 0.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      840/    1500 | consumed samples:       161280 | elapsed time per iteration (ms): 1268.0 | learning rate: 2.663E-04 | global batch size:   192 | lm loss: 1.526459E+00 | loss scale: 65536.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      850/    1500 | consumed samples:       163200 | elapsed time per iteration (ms): 1273.1 | learning rate: 2.603E-04 | global batch size:   192 | lm loss: 1.510167E+00 | loss scale: 65536.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      860/    1500 | consumed samples:       165120 | elapsed time per iteration (ms): 1267.1 | learning rate: 2.542E-04 | global batch size:   192 | lm loss: 1.504080E+00 | loss scale: 65536.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      870/    1500 | consumed samples:       167040 | elapsed time per iteration (ms): 1268.2 | learning rate: 2.482E-04 | global batch size:   192 | lm loss: 1.528673E+00 | loss scale: 65536.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      880/    1500 | consumed samples:       168960 | elapsed time per iteration (ms): 1266.7 | learning rate: 2.421E-04 | global batch size:   192 | lm loss: 1.503616E+00 | loss scale: 65536.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      890/    1500 | consumed samples:       170880 | elapsed time per iteration (ms): 1347.9 | learning rate: 2.361E-04 | global batch size:   192 | lm loss: 1.497667E+00 | loss scale: 65536.0 | grad norm: 0.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      900/    1500 | consumed samples:       172800 | elapsed time per iteration (ms): 1404.2 | learning rate: 2.301E-04 | global batch size:   192 | lm loss: 1.495017E+00 | loss scale: 65536.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      910/    1500 | consumed samples:       174720 | elapsed time per iteration (ms): 1265.2 | learning rate: 2.241E-04 | global batch size:   192 | lm loss: 1.514772E+00 | loss scale: 65536.0 | grad norm: 0.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      920/    1500 | consumed samples:       176640 | elapsed time per iteration (ms): 1266.5 | learning rate: 2.181E-04 | global batch size:   192 | lm loss: 1.482036E+00 | loss scale: 65536.0 | grad norm: 0.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      930/    1500 | consumed samples:       178560 | elapsed time per iteration (ms): 1266.8 | learning rate: 2.121E-04 | global batch size:   192 | lm loss: 1.469814E+00 | loss scale: 65536.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      940/    1500 | consumed samples:       180480 | elapsed time per iteration (ms): 1266.6 | learning rate: 2.061E-04 | global batch size:   192 | lm loss: 1.478214E+00 | loss scale: 65536.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      950/    1500 | consumed samples:       182400 | elapsed time per iteration (ms): 1271.0 | learning rate: 2.002E-04 | global batch size:   192 | lm loss: 1.463557E+00 | loss scale: 65536.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      960/    1500 | consumed samples:       184320 | elapsed time per iteration (ms): 1269.3 | learning rate: 1.943E-04 | global batch size:   192 | lm loss: 1.461920E+00 | loss scale: 65536.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      970/    1500 | consumed samples:       186240 | elapsed time per iteration (ms): 1285.3 | learning rate: 1.884E-04 | global batch size:   192 | lm loss: 1.450261E+00 | loss scale: 65536.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      980/    1500 | consumed samples:       188160 | elapsed time per iteration (ms): 1272.3 | learning rate: 1.826E-04 | global batch size:   192 | lm loss: 1.442138E+00 | loss scale: 65536.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      990/    1500 | consumed samples:       190080 | elapsed time per iteration (ms): 1267.4 | learning rate: 1.768E-04 | global batch size:   192 | lm loss: 1.480331E+00 | loss scale: 65536.0 | grad norm: 0.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1000/    1500 | consumed samples:       192000 | elapsed time per iteration (ms): 1267.2 | learning rate: 1.710E-04 | global batch size:   192 | lm loss: 1.446659E+00 | loss scale: 65536.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4828.87, 4829.22)
------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 1.353746E+00 | lm loss PPL: 3.871903E+00 | 
------------------------------------------------------------------------------------------------
saving checkpoint at iteration    1000 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration    1000 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3068.45, 3068.47)
 iteration     1010/    1500 | consumed samples:       193920 | elapsed time per iteration (ms): 1267.4 | learning rate: 1.653E-04 | global batch size:   192 | lm loss: 1.437428E+00 | loss scale: 65536.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1020/    1500 | consumed samples:       195840 | elapsed time per iteration (ms): 1269.6 | learning rate: 1.597E-04 | global batch size:   192 | lm loss: 1.432474E+00 | loss scale: 131072.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1030/    1500 | consumed samples:       197760 | elapsed time per iteration (ms): 1268.6 | learning rate: 1.541E-04 | global batch size:   192 | lm loss: 1.433856E+00 | loss scale: 131072.0 | grad norm: 0.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1040/    1500 | consumed samples:       199680 | elapsed time per iteration (ms): 1268.1 | learning rate: 1.485E-04 | global batch size:   192 | lm loss: 1.459827E+00 | loss scale: 131072.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1050/    1500 | consumed samples:       201600 | elapsed time per iteration (ms): 1269.4 | learning rate: 1.430E-04 | global batch size:   192 | lm loss: 1.433295E+00 | loss scale: 131072.0 | grad norm: 0.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1060/    1500 | consumed samples:       203520 | elapsed time per iteration (ms): 1276.6 | learning rate: 1.376E-04 | global batch size:   192 | lm loss: 1.420597E+00 | loss scale: 131072.0 | grad norm: 0.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1070/    1500 | consumed samples:       205440 | elapsed time per iteration (ms): 1268.1 | learning rate: 1.322E-04 | global batch size:   192 | lm loss: 1.420856E+00 | loss scale: 131072.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1080/    1500 | consumed samples:       207360 | elapsed time per iteration (ms): 1268.1 | learning rate: 1.269E-04 | global batch size:   192 | lm loss: 1.450218E+00 | loss scale: 131072.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1090/    1500 | consumed samples:       209280 | elapsed time per iteration (ms): 1267.6 | learning rate: 1.217E-04 | global batch size:   192 | lm loss: 1.422867E+00 | loss scale: 131072.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1100/    1500 | consumed samples:       211200 | elapsed time per iteration (ms): 1268.4 | learning rate: 1.166E-04 | global batch size:   192 | lm loss: 1.415868E+00 | loss scale: 131072.0 | grad norm: 0.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1110/    1500 | consumed samples:       213120 | elapsed time per iteration (ms): 1269.9 | learning rate: 1.115E-04 | global batch size:   192 | lm loss: 1.410830E+00 | loss scale: 131072.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1120/    1500 | consumed samples:       215040 | elapsed time per iteration (ms): 1268.3 | learning rate: 1.065E-04 | global batch size:   192 | lm loss: 1.399008E+00 | loss scale: 131072.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1130/    1500 | consumed samples:       216960 | elapsed time per iteration (ms): 1269.3 | learning rate: 1.016E-04 | global batch size:   192 | lm loss: 1.410886E+00 | loss scale: 131072.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1140/    1500 | consumed samples:       218880 | elapsed time per iteration (ms): 1269.1 | learning rate: 9.677E-05 | global batch size:   192 | lm loss: 1.399401E+00 | loss scale: 131072.0 | grad norm: 0.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1150/    1500 | consumed samples:       220800 | elapsed time per iteration (ms): 1268.0 | learning rate: 9.204E-05 | global batch size:   192 | lm loss: 1.395944E+00 | loss scale: 131072.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1160/    1500 | consumed samples:       222720 | elapsed time per iteration (ms): 1270.4 | learning rate: 8.741E-05 | global batch size:   192 | lm loss: 1.415350E+00 | loss scale: 131072.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1170/    1500 | consumed samples:       224640 | elapsed time per iteration (ms): 1268.6 | learning rate: 8.287E-05 | global batch size:   192 | lm loss: 1.392385E+00 | loss scale: 131072.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1180/    1500 | consumed samples:       226560 | elapsed time per iteration (ms): 1269.6 | learning rate: 7.842E-05 | global batch size:   192 | lm loss: 1.402041E+00 | loss scale: 131072.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1190/    1500 | consumed samples:       228480 | elapsed time per iteration (ms): 1268.1 | learning rate: 7.408E-05 | global batch size:   192 | lm loss: 1.393271E+00 | loss scale: 131072.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1200/    1500 | consumed samples:       230400 | elapsed time per iteration (ms): 1268.5 | learning rate: 6.984E-05 | global batch size:   192 | lm loss: 1.411712E+00 | loss scale: 131072.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4823.52, 4823.68)
------------------------------------------------------------------------------------------------
 validation loss at iteration 1200 | lm loss value: 1.339522E+00 | lm loss PPL: 3.817218E+00 | 
------------------------------------------------------------------------------------------------
saving checkpoint at iteration    1200 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration    1200 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3028.76, 3028.81)
 iteration     1210/    1500 | consumed samples:       232320 | elapsed time per iteration (ms): 1266.3 | learning rate: 6.570E-05 | global batch size:   192 | lm loss: 1.395917E+00 | loss scale: 131072.0 | grad norm: 0.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1220/    1500 | consumed samples:       234240 | elapsed time per iteration (ms): 1999.9 | learning rate: 6.168E-05 | global batch size:   192 | lm loss: 1.366733E+00 | loss scale: 131072.0 | grad norm: 0.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1230/    1500 | consumed samples:       236160 | elapsed time per iteration (ms): 1659.2 | learning rate: 5.776E-05 | global batch size:   192 | lm loss: 1.402292E+00 | loss scale: 131072.0 | grad norm: 0.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1240/    1500 | consumed samples:       238080 | elapsed time per iteration (ms): 2547.9 | learning rate: 5.395E-05 | global batch size:   192 | lm loss: 1.370003E+00 | loss scale: 131072.0 | grad norm: 0.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1250/    1500 | consumed samples:       240000 | elapsed time per iteration (ms): 1447.6 | learning rate: 5.026E-05 | global batch size:   192 | lm loss: 1.392296E+00 | loss scale: 131072.0 | grad norm: 0.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1260/    1500 | consumed samples:       241920 | elapsed time per iteration (ms): 1262.9 | learning rate: 4.669E-05 | global batch size:   192 | lm loss: 1.383876E+00 | loss scale: 131072.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1270/    1500 | consumed samples:       243840 | elapsed time per iteration (ms): 1263.7 | learning rate: 4.323E-05 | global batch size:   192 | lm loss: 1.405136E+00 | loss scale: 131072.0 | grad norm: 0.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1280/    1500 | consumed samples:       245760 | elapsed time per iteration (ms): 1270.2 | learning rate: 3.989E-05 | global batch size:   192 | lm loss: 1.389619E+00 | loss scale: 131072.0 | grad norm: 0.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1290/    1500 | consumed samples:       247680 | elapsed time per iteration (ms): 1265.6 | learning rate: 3.668E-05 | global batch size:   192 | lm loss: 1.386336E+00 | loss scale: 131072.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1300/    1500 | consumed samples:       249600 | elapsed time per iteration (ms): 1266.3 | learning rate: 3.359E-05 | global batch size:   192 | lm loss: 1.365987E+00 | loss scale: 131072.0 | grad norm: 0.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1310/    1500 | consumed samples:       251520 | elapsed time per iteration (ms): 1266.4 | learning rate: 3.063E-05 | global batch size:   192 | lm loss: 1.378116E+00 | loss scale: 131072.0 | grad norm: 0.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1320/    1500 | consumed samples:       253440 | elapsed time per iteration (ms): 1266.6 | learning rate: 2.780E-05 | global batch size:   192 | lm loss: 1.388882E+00 | loss scale: 131072.0 | grad norm: 0.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1330/    1500 | consumed samples:       255360 | elapsed time per iteration (ms): 1267.3 | learning rate: 2.510E-05 | global batch size:   192 | lm loss: 1.365652E+00 | loss scale: 131072.0 | grad norm: 0.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1340/    1500 | consumed samples:       257280 | elapsed time per iteration (ms): 1266.2 | learning rate: 2.252E-05 | global batch size:   192 | lm loss: 1.383442E+00 | loss scale: 131072.0 | grad norm: 0.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1350/    1500 | consumed samples:       259200 | elapsed time per iteration (ms): 1276.7 | learning rate: 2.008E-05 | global batch size:   192 | lm loss: 1.378329E+00 | loss scale: 131072.0 | grad norm: 0.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1360/    1500 | consumed samples:       261120 | elapsed time per iteration (ms): 1266.9 | learning rate: 1.778E-05 | global batch size:   192 | lm loss: 1.381264E+00 | loss scale: 131072.0 | grad norm: 0.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1370/    1500 | consumed samples:       263040 | elapsed time per iteration (ms): 1267.1 | learning rate: 1.561E-05 | global batch size:   192 | lm loss: 1.378576E+00 | loss scale: 131072.0 | grad norm: 0.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1380/    1500 | consumed samples:       264960 | elapsed time per iteration (ms): 1268.1 | learning rate: 1.358E-05 | global batch size:   192 | lm loss: 1.385279E+00 | loss scale: 131072.0 | grad norm: 0.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1390/    1500 | consumed samples:       266880 | elapsed time per iteration (ms): 1267.9 | learning rate: 1.168E-05 | global batch size:   192 | lm loss: 1.374005E+00 | loss scale: 131072.0 | grad norm: 0.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1400/    1500 | consumed samples:       268800 | elapsed time per iteration (ms): 1275.4 | learning rate: 9.927E-06 | global batch size:   192 | lm loss: 1.376238E+00 | loss scale: 131072.0 | grad norm: 0.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    evaluate .......................................: (4847.58, 4847.92)
------------------------------------------------------------------------------------------------
 validation loss at iteration 1400 | lm loss value: 1.312738E+00 | lm loss PPL: 3.716334E+00 | 
------------------------------------------------------------------------------------------------
saving checkpoint at iteration    1400 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration    1400 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
(min, max) time across ranks (ms):
    save-checkpoint ................................: (3044.48, 3044.50)
 iteration     1410/    1500 | consumed samples:       270720 | elapsed time per iteration (ms): 1268.6 | learning rate: 8.311E-06 | global batch size:   192 | lm loss: 1.364487E+00 | loss scale: 131072.0 | grad norm: 0.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1420/    1500 | consumed samples:       272640 | elapsed time per iteration (ms): 1268.3 | learning rate: 6.837E-06 | global batch size:   192 | lm loss: 1.364972E+00 | loss scale: 131072.0 | grad norm: 0.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1430/    1500 | consumed samples:       274560 | elapsed time per iteration (ms): 1269.2 | learning rate: 5.505E-06 | global batch size:   192 | lm loss: 1.387737E+00 | loss scale: 131072.0 | grad norm: 0.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1440/    1500 | consumed samples:       276480 | elapsed time per iteration (ms): 1268.4 | learning rate: 4.316E-06 | global batch size:   192 | lm loss: 1.360306E+00 | loss scale: 131072.0 | grad norm: 0.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1450/    1500 | consumed samples:       278400 | elapsed time per iteration (ms): 1268.6 | learning rate: 3.270E-06 | global batch size:   192 | lm loss: 1.359615E+00 | loss scale: 131072.0 | grad norm: 0.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1460/    1500 | consumed samples:       280320 | elapsed time per iteration (ms): 1305.0 | learning rate: 2.368E-06 | global batch size:   192 | lm loss: 1.376009E+00 | loss scale: 131072.0 | grad norm: 0.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1470/    1500 | consumed samples:       282240 | elapsed time per iteration (ms): 1268.2 | learning rate: 1.611E-06 | global batch size:   192 | lm loss: 1.382726E+00 | loss scale: 131072.0 | grad norm: 0.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1480/    1500 | consumed samples:       284160 | elapsed time per iteration (ms): 1269.1 | learning rate: 9.987E-07 | global batch size:   192 | lm loss: 1.356927E+00 | loss scale: 131072.0 | grad norm: 0.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1490/    1500 | consumed samples:       286080 | elapsed time per iteration (ms): 1270.8 | learning rate: 5.320E-07 | global batch size:   192 | lm loss: 1.352815E+00 | loss scale: 131072.0 | grad norm: 0.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1500/    1500 | consumed samples:       288000 | elapsed time per iteration (ms): 1269.0 | learning rate: 2.109E-07 | global batch size:   192 | lm loss: 1.371440E+00 | loss scale: 131072.0 | grad norm: 0.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-12-09 10:47:12 
saving checkpoint at iteration    1500 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
  successfully saved checkpoint at iteration    1500 to /scratch/user/siweicui/Megatron-LM/experiments/codeparrot-small
Evaluating on 1920 samples
Evaluating iter 1/10
Evaluating iter 2/10
Evaluating iter 3/10
Evaluating iter 4/10
Evaluating iter 5/10
Evaluating iter 6/10
Evaluating iter 7/10
Evaluating iter 8/10
Evaluating iter 9/10
Evaluating iter 10/10
(min, max) time across ranks (ms):
    evaluate .......................................: (4825.66, 4825.93)
------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 on validation set | lm loss value: 1.297939E+00 | lm loss PPL: 3.661741E+00 | 
------------------------------------------------------------------------------------------------------------------
Evaluating on 1920 samples
Evaluating iter 1/10
Evaluating iter 2/10
Evaluating iter 3/10
Evaluating iter 4/10
Evaluating iter 5/10
Evaluating iter 6/10
Evaluating iter 7/10
Evaluating iter 8/10
Evaluating iter 9/10
Evaluating iter 10/10
(min, max) time across ranks (ms):
    evaluate .......................................: (4839.68, 4839.95)
------------------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 on test set | lm loss value: 1.368919E+00 | lm loss PPL: 3.931100E+00 | 
------------------------------------------------------------------------------------------------------------
